{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача поиск похожих по эмбеддингам\n",
    "Скачиваем датасет (источник): положительные, отрицательные.\n",
    "или можно через ноутбук\n",
    "\n",
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "\n",
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
    "\n",
    "что надо сделать \n",
    "\n",
    "1. объединить в одну выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maximdoroshenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @first_timee хоть я и школота, но поверь, у на...\n",
       "1  Да, все-таки он немного похож на него. Но мой ...\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. на основе word2vec/fasttext/glove/слоя Embedding реализовать метод поиска ближайших твитов\n",
    "(на вход метода должен приходить запрос (какой-то твит, вопрос) и количество вариантов вывода к примеру 5-ть, ваш метод должен возвращать 5-ть ближайших твитов к этому запросу)\n",
    "3. Проверить насколько хорошо работают подходы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/maximdoroshenko/opt/anaconda3/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/maximdoroshenko/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/maximdoroshenko/opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/maximdoroshenko/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.22.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set(stopwords.words(\"russian\"))\n",
    "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# Remove duplicated after preprocessing\n",
    "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
    "df = df.iloc[idx, :]\n",
    "\n",
    "# Remove empty values and keep relevant columns\n",
    "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "\n",
    "docs = df[\"text\"].values\n",
    "tokenized_docs = df[\"tokens\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43269</th>\n",
       "      <td>@00000Asrova кто знает... :D А вообще, это кто...</td>\n",
       "      <td>[00000asrova, кто, знает, вообще, это, кто, был]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23246</th>\n",
       "      <td>00:00.....пойти уроки что ль поделать..)))))))...</td>\n",
       "      <td>[0000пойти, уроки, что, ль, поделать, httptcop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45781</th>\n",
       "      <td>@000Nana000 заправил систему кипяточком))) и в...</td>\n",
       "      <td>[000nana000, заправил, систему, кипяточком, вп...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7460</th>\n",
       "      <td>@000Nana000 Конечно! Зачем нам неудачники ))) ...</td>\n",
       "      <td>[000nana000, конечно, зачем, нам, неудачники, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>@009_panda да надо скоро идти, но все никак не...</td>\n",
       "      <td>[009panda, да, надо, скоро, идти, но, все, ник...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "43269  @00000Asrova кто знает... :D А вообще, это кто...   \n",
       "23246  00:00.....пойти уроки что ль поделать..)))))))...   \n",
       "45781  @000Nana000 заправил систему кипяточком))) и в...   \n",
       "7460   @000Nana000 Конечно! Зачем нам неудачники ))) ...   \n",
       "24326  @009_panda да надо скоро идти, но все никак не...   \n",
       "\n",
       "                                                  tokens  \n",
       "43269   [00000asrova, кто, знает, вообще, это, кто, был]  \n",
       "23246  [0000пойти, уроки, что, ль, поделать, httptcop...  \n",
       "45781  [000nana000, заправил, систему, кипяточком, вп...  \n",
       "7460   [000nana000, конечно, зачем, нам, неудачники, ...  \n",
       "24326  [009panda, да, надо, скоро, идти, но, все, ник...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217309, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "    \n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.30283317, -0.20712417, -0.3462995 , -0.06517202, -0.3404316 ,\n",
       "         0.6041342 ,  0.43395656,  0.42605087,  0.00391609, -0.2760664 ,\n",
       "        -0.50516784, -0.23609805,  0.07685451,  0.5344544 , -0.13421503,\n",
       "        -0.07629161,  0.11465648, -0.691084  , -0.40536916,  0.09282611,\n",
       "        -0.04479497,  0.47768867, -0.01528123,  0.44763947,  0.28622422,\n",
       "         0.28387883, -0.26889616,  0.16918483, -0.19874267, -0.24481964,\n",
       "         0.00242973, -0.32306582,  0.37143135,  0.32551086,  0.05335397,\n",
       "         0.26398084,  0.33884817, -0.9141328 , -0.31157762,  0.01555198,\n",
       "         0.3320907 ,  0.07094458,  0.66456115, -0.06313147,  0.23505297,\n",
       "         0.1222454 ,  0.24011584,  0.16154976,  0.5486938 , -0.6217685 ,\n",
       "         0.14902207, -0.83998555,  0.18760987,  0.2313182 ,  0.4972492 ,\n",
       "        -0.04477295, -0.48122576,  0.20603289,  0.14241582,  0.5108996 ,\n",
       "        -0.17002346, -0.16331035, -0.47614282,  0.27799818,  0.17010558,\n",
       "         0.14264111,  0.09437311, -0.29626024, -0.15102316, -0.39392862,\n",
       "         0.056601  , -0.01688084, -0.31570062, -0.43126845,  0.01513424,\n",
       "        -0.02038817,  0.12949827, -0.3065202 , -0.42246538, -0.25269097,\n",
       "         0.0644673 , -0.1612905 , -0.22180524, -0.06837145, -0.3782585 ,\n",
       "         0.31437185, -0.07042391, -0.58065456, -0.20970154, -0.17379133,\n",
       "        -0.5479326 ,  0.81106824, -0.05126208, -0.09864664,  0.5517785 ,\n",
       "        -0.2472887 ,  0.12131945,  0.25293317, -0.36561778, -0.13679282],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = 'заправил машину кипяточком'\n",
    "test_tokenized = clean_text(test_text, word_tokenize, custom_stopwords)\n",
    "test_vec = vectorize([test_tokenized], model=model)\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('машину', 1.0),\n",
       " ('вокзал', 0.9892905950546265),\n",
       " ('рабочую', 0.9890004396438599),\n",
       " ('квартиру', 0.9887105822563171),\n",
       " ('мексике', 0.9887104034423828)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(test_vec[0], topn=5, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "X = np.vstack(vectorized_docs)\n",
    "tree = KDTree(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ого!!!бачила тікишо на обочині траси згорівшу машину(',\n",
       " 'обида.(\\nбилеты на елку не дают больше...(((\\nбольшая стала:3',\n",
       " 'в магазине нет редбулла((( зачем вышел? #жизньболь',\n",
       " 'Тюменская челядь в очереди за халявкой))) http://t.co/WiL58G1rJv',\n",
       " '@igor_l33t 8  Тысяч то хватило?) и всех на кино пустили?:D']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, ind = tree.query(test_vec, k=5)\n",
    "[df.text.iloc[i] for i in ind[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в магазине нет редбулла((( зачем вышел? #жизньболь',\n",
       " 'Убираться в квартире за новые наушники вполне норм)',\n",
       " 'Оф,еще кто то второго числа ходит за игрушками((бля',\n",
       " 'пришла на кухню именно в ту минуту,когда по телеку шла реклама шуб за 1488 рублей в месяц. неонацисты рекламу теперь у нас делают?)',\n",
       " 'обида.(\\nбилеты на елку не дают больше...(((\\nбольшая стала:3',\n",
       " 'Опять потеряла где-то в доме документы на машину. Офигеть я клуша((',\n",
       " 'Решили сходить в магазин, как оказалась не один в округе не работает и тут мы поняли, что даже магазины против нас.жизньболь(',\n",
       " '@r3nya гугль решил только 16Гб мимо плейстора продавать :(',\n",
       " '@Evgeniya996 реклама была и сказали что 12 числа будет развязка)',\n",
       " 'Еще не успели собрать ёлку на площади, а она уже упала)))']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = 'в магазине нет редбулла((( зачем вышел? #жизньболь'\n",
    "test_tokenized = clean_text(test_text, word_tokenize, custom_stopwords)\n",
    "test_vec = vectorize([test_tokenized], model=model)\n",
    "\n",
    "_, ind = tree.query(test_vec, k=10)\n",
    "[df.text.iloc[i] for i in ind[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFT = FastText(sentences=tokenized_docs, vector_size=300, min_count=1, window=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в магазине нет редбулла((( зачем вышел? #жизньболь',\n",
       " 'И где этот   : Я уже дебиан установила! :(',\n",
       " 'Ездили в триумф и в хэппимол - купили только одну юбку;(жизнь боль',\n",
       " 'Хершеля убил Филипп, сижу и плачу. Зато Филиппа убили и, может, Джудит, маленькая, умерла. Одни разочарования :((  \\n#TWD',\n",
       " '@chernyatkina я заебался у кабинета стоять))у тебя мама у кабинета стоит)',\n",
       " '@Tritatushe4ka моя симка после смерти мужа вышла замуж за робота семьи.. но от горя стала шлюхой и изменяет ему=((',\n",
       " 'я выключил вкладку в которой играла музыка:((((9\\nсук блеа',\n",
       " 'Кашель заипал...уже и на грелках лежала и картошку клали :(',\n",
       " 'Ездила в центр переливания крови...решилась все-таки,сдала свою третью группу))',\n",
       " 'Айфон еле восстановил чуть с ума не сошел(']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_docs = vectorize(tokenized_docs, model=modelFT)\n",
    "X = np.vstack(vectorized_docs)\n",
    "tree = KDTree(X)\n",
    "\n",
    "test_text = 'в магазине нет редбулла((( зачем вышел? #жизньболь'\n",
    "test_tokenized = clean_text(test_text, word_tokenize, custom_stopwords)\n",
    "test_vec = vectorize([test_tokenized], model=modelFT)\n",
    "\n",
    "_, ind = tree.query(test_vec, k=10)\n",
    "[df.text.iloc[i] for i in ind[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
